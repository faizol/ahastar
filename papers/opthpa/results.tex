\section{Results}
\label{sec-results}
We begin with Table \ref{table-graphsize} which shows the average size
of the state space across our two benchmarks: Baldur's Gate (henceforth BG) and 
R0-R20 (the three variants of the map from Figure \ref{fig-contrast}).
We compare the number of nodes and edges in the original versions of the maps
with the number of nodes and edges in our modified maps where symmetric paths 
have been pruned.
\input graphsize
Notice that in each case the modified maps are significantly smaller.
For example, on R0 over 61\% of all nodes and xx\% of all edges required are pruned.
On R10 and R20 the total saving is less; between 4-14\% of all nodes and xx-xx\% of all edges are pruned.  
Recall that on these maps each traversable tile has either a 10\% or 20\% chance of becoming an obstacle. 
This changes the topography of the map significantly; there are much fewer 
symmetric paths and the empty rooms identified are consequently much smaller -- as expected.
Meanwhile, on the BG maps, 34\% of nodes and xx\% of edges are pruned.
As we will see this is sufficient to achieve a two-fold speedup however it seems our decomposition
is significantly less effective on these instances than R0.
We attribute the difference to the odd 45-degree orientation of the BG maps.
Upon investigation it was discovered that in such cases our decomposition technique tends to identify very 
long narrow rooms with few interior nodes. 
We expect that a more sophisticated decomposition technique would yield significantly better results
in these cases.
\par
Thus smaller rooms tend to be produced on the BG problem set (when compared to R0) leading to 
fewer interior nodes that OHA* can prune. 
We expect that a more sophisticated decomposition technique method would yield significantly 
better results.
Looking at the results for our demo map (R10-R50 in particular) we observe that as the number of 
randomly distributed obstacles increases there is a corresponding drop in the number of nodes that may be 
pruned.
This is as expected. 
On R50 for example most rooms contain few if any interior nodes; 
we see that OHA* can only prune 8\% of all nodes and 49\% of all edges required by A*.
\par
We begin with Figure \ref{fig-searcheffort} where we examine a number of metrics
related the average search effort of the two algorithms on the BG problem set.
In all cases we present results with respect to optimal path length.
\begin{figure}[t]
	\begin{center}
		       \includegraphics[width=0.95\columnwidth, trim = 20mm 17mm 20mm 5mm]{diagrams/bg_effort.pdf}
	\end{center}
	\caption{Average performance of A* and OHA*. We look at average nodes expanded, nodes generated,
	peak memory and search times.}
	\label{fig-searcheffort}
\end{figure}
\par \indent
Figures \ref{fig-searcheffort}(a) and \ref{fig-searcheffort}(b)  show the average number of 
nodes expanded and generated. 
The trend is identical in both cases; OHA* expands and generates between 29-41\% fewer nodes than A*.
Problem instances with longer path lengths yield greater savings as on larger maps there is more 
opportunity for OHA* to prune nodes. 
Figure \ref{fig-searcheffort}(c) shows the corresponding search times for the problem instances. 
The same general trend may be observed; OHA* is between 1.7-2.1 times faster than A*.
We believe part of this speedup is attributable to the smaller open list OHA* needs to maintain.
We present data to support this hypothesis in Figure \ref{fig-searcheffort}(d) where we measure 
the maximum size of the open list. 
Notice that in the worst case OHA* is shown to maintain between 12-37\% fewer nodes where each
maintenance operation requires logarithmic time in the number of nodes.
\par
Finally, we turn our attention to Table 2 where we measure the 
performance of A* and OHA* on the six variants of the demo map.
Our intention here is examine how the performance of OHA* degrades in scenarios with
high obstacle density.

\begin{figure}[t]
	\begin{center}
		       \includegraphics[width=0.9\columnwidth, trim = 20mm 17mm 20mm 5mm]{diagrams/csc2f_performance.pdf}
	\end{center}
	\caption{Average performance of A* on the demo map from Figure \ref{fig-contrast}. On the left
	we show the average proportion of nodes expanded by A\* on our modified grid map vs. the original map.
	On the right we show the average speedup experienced by A\* on our modified grid map vs. the original map. 
	}
\label{fig-csc2fresults}
\end{figure}


Observe that as the number of obstacles increases the number of nodes expanded by OHA*, and the
associated search time, both increase. 
The drop in performance from R0 to R10 is quite significant; OHA*'s search times drop
from approximately 3.3 times faster than A* to just 1.36 times faster.
This advantage is further eroded as we go from R10 to R20 where OHA* is only 
1.23 times faster than A*.
Beyond this point OHA*'s map decomposition yields little advantage; 
as its performance approaches the worst case the algorithm collapses
to standard A*.
\par
Summarising our results: we have seen that OHA* expands, on average, between 40-70\% less nodes than A*.
We solve a large set of realistic problems optimally and achieve an average speedup of between 1.7 to 
3.3 times faster than A*.
In the worst case we have shown that the performance of OHA* collapses to that of A*.
